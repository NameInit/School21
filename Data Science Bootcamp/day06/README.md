# SQL and Pandas

Summary: Today we will help you acquire skills with SQL.

ðŸ’¡ [Tap here](https://new.oprosso.net/p/4cb31ec3f47a4596bc758ea1861fb624) **to leave your feedback on the project**. It's anonymous and will help our team make your educational experience better. We recommend completing the survey immediately after the project.

## Contents

1. [Chapter I](#chapter-i) \
    1.1. [Foreword](#foreword)
2. [Chapter II](#chapter-ii) \
    2.1. [Instructions](#instructions)
3. [Chapter III](#chapter-iii) \
    3.1. [Specific instructions for the day](#specific-instructions-for-the-day)
4. [Chapter IV](#chapter-iv) \
    4.1. [Exercise 00. Select](#exercise-00-select)
5. [Chapter V](#chapter-v) \
    5.1. [Exercise 01. Subquery](#exercise-01-subquery)
6. [Chapter VI](#chapter-vi) \
    6.1. [Exercise 02. Join](#exercise-02-join)
7. [Chapter VII](#chapter-vii) \
    7.1. [Exercise 03. Aggregations](#exercise-03-aggregations)
8. [Chapter VIII](#chapter-viii) \
    8.1. [Exercise 04. A/B-testing](#exercise-04-a/b-testing)
    
   
## Chapter I

### Foreword

Is it "Sequel" or "Ess Kyew Ell"? How do you pronounce SQL correctly? Some interviewers reject interviewees if they don't pronounce it the "right" way. Apparently, knowing the correct pronunciation is really important.

SQL was originally spelled SEQUEL (Structured English Query Language), but it was later discovered that this was the registered trademark of an aircraft company. The authors had to change it. Since then, it has been SQL (Structured Query Language). One of the authors [was asked](http://patorjk.com/blog/2012/01/26/pronouncing-sql-s-q-l-or-sequel/) which pronunciation is correct. He answered:

```
Since the language was originally named SEQUEL, many people continued to pronounce the name that way after it was shortened to SQL. Both pronunciations are widely used and recognized. As for which is more "official", I guess the authority would be the ISO standard, which is spelled (and presumably pronounced) "S-Q-L".

Thanks for your interest!

â€” Don Chamberlin
```

One of the authors said that you can use either method. "S-Q-L" is supposedly the more official way of doing it. What do other "authority figures" think?

Apparently, the official pronunciation of "MySQL" is "My Ess Kyew Ell" (not "my sequel"). However, the official Oracle documentation says that the correct pronunciation is "Sequel."

Before diving into the exercises, you must make an important choice: Are you going to pronounce it "Sequel" or "S-Q-L"? More informal or more formal? Where does your heart lie?

## Chapter II

### Instructions

* Use this page as your only reference. Do not pay attention to rumors or speculation about how to prepare your solution.
* Here and throughout, we use Python 3 as the only correct version of Python.
* The python files for python exercises (module01, module02, module03) must have the following block at the end: `if __name__ == â€˜__main__â€™`.
* Pay attention to the permissions of your files and directories.
* To be assessed your solution must be in your GIT repository.
* Your solutions will be evaluated by your peers in the bootcamp.
* You should not leave any other files in your directory other than those explicitly specified in the exercise instructions. It is recommended that you modify your .gitignore to avoid any accidents.
* Your solution must be in your GIT repository for evaluation. Always push only to the develop branch! The master branch will be ignored. Work in the src directory.
* When you need to get precise output in your programs, it is forbidden to display a precalculated output instead of performing the exercise correctly.
* Have a question? Ask your neighbor on the right. If that fails, try your neighbor on the left.
* Your reference materials are your peers, the internet, and Google.
* Read the examples carefully. They may require information that is not specified elsewhere in the subject.
* May the Force be with you!

## Chapter III

### Specific instructions for the day

* Use the Jupyter notebook to work with your code.
* For each major subtask in the list of exercises, your ipynb file should have an H2 heading to help your peers easily navigate your code.
* No imports are allowed except those mentioned in the "Authorized Functions" section of the title block of each exercise.
* You can use any built-in function as long as it is not prohibited by the exercise.
* Today, you may only use the following Pandas methods: `io.sql.read_sql` and `to_sql`, unless otherwise prescribed in the exercise.
* Save and load all required data in the `data/` subfolder.

## Chapter IV

### Exercise 00. Select

- Turn-in directory: `ex00/`.
- Files to turn in: `ex00_first_select.ipynb`.
- Allowed functions: `import pandas as pd`, `import sqlite3`.

Having only Python in your toolbox can sometimes be limiting. Remember during the first day of Piscine when you worked with command-line tools because they are more efficient for certain tasks? Today, you will work with SQL. Why might that be useful to you? Sometimes your data might not be stored in convenient CSV or JSON files, but rather in a database, and you have to extract it somehow. SQL can also be used with Apache Spark and Hive, tools used for processing big data, but with Hive, it's called HQL.

Download the [the SQLite database](https://drive.google.com/open?id=1zQ8AR2Ry3ajzB3UZO1Sfk3xtDJlzQF2M). Throughout the day, you will work with different tables from the database using Pandas. They are all connected and refer to the same project. The dataset is real and comes from an educational company. The company has its own platform where students can check if their solution is correct and receive other feedback. The table checker stores logs of which labs and when users checked them.

![lada](misc/images/lada.png)

The company created a new page on the platform called Newsfeed where these logs are visible to all students in the program. Logs of page visits are stored in another table, pageviews. The hypothesis was that the page would create peer pressure, prompting students to start working on the labs earlier. This could be beneficial because they could complete more iterations and experiment with different approaches. In this series of exercises, you will determine whether this hypothesis is correct.

![lenta](misc/images/lenta.png)

But first, let's start with something simple. For this exercise, you need to retrieve filtered data from a table in the database. Why is it important to filter the data in the query but not afterward in Pandas? Because tables can be enormous. If you try to retrieve the entire table, you won't be able to process it. Always keep this in mind.

The first method of filtering is to choose only the columns you need.

Second, choose the rows you really need.

In more detail:

1. Put the database in the data subfolder in the src directory.
2. Create a connection to the database using the sqlite3 library.
3. Get the schema of the `pageviews` table using `pd.io.sql.read_sql()` and the query `"PRAGMA table_info(pageviews);"`.
4. Get only the first ten rows of the `pageviews` table to see what the table looks like.
5. Get the subtable using one query where:
   - Only "uid" and "datetime" are used.
   - Only user data (`user_*`), not admin data, is used.
   - It is sorted by "uid" in ascending order.
   - The index column is "datetime".
   - "datetime" is converted to a `DatetimeIndex`.
   - The name of the dataframe is `pageviews`.
6. Close the connection to the database.

## Chapter V

### Exercise 01. Subquery

- Turn-in directory: `ex01/`.
- Files to turn in: `ex01_subquery.ipynb`.
- Allowed functions: `import pandas as pd`, `import sqlite3`.

Okay, let's make something more complicated. Have you heard of subqueries? It's like a query inside a query. How could they be useful to you? Generally, you might want to perform aggregations on a select that you had previously made. Note, however, that nested queries run first, followed by the main query.

Here's what you need to do:

1. Create a connection to the database using the `sqlite3` library.
2. Get the schema of the `checker` table.
3. Get only the first ten rows of the table to see what the table looks like.
4. Count how many rows satisfy the following conditions using one query with any number of subqueries.
   - Count the rows from the `pageviews` table, but only those from the `checker` table with:
     - `status = 'ready'`, we do not want to analyze logs that are
    status checking.
     - `numTrials = 1`, we only want to analyze the first commits because they tell us when a student started working on a lab.
     - "labnames" should be from the list: `laba04`, `laba04s`, `laba05`, `laba06`, `laba06s`, and `project1`. Only these were active during the experiment.
   - Store the checkers in the dataframe with the column "cnt".
5. Close the connection.

## Chapter VI

### Exercise 02. Join

- Turn-in directory: `ex02/`.
- Files to turn in: `ex02_joins.ipynb`.
- Allowed functions: `import pandas as pd`, `import sqlite3`.

In this exercise, you will create a so-called datamart. A datamart is a table used for analytics. It is usually created by joining different tables together. For this exercise, we will collect various pieces of data about our users, such as when they made their first commits and when they visited the newsfeed for the first time. This will help us analyze the data later.

Read the full task to see what you need to do in this exercise:

1. Create a connection to the database using the `sqlite3` library.
2. Create a new table called `datamart` in the database by joining the tables `pageviews` and `checker` using only one query.
   - The table should have the following columns: "uid", "labname", "first_commit_ts", and "first_view_ts".
   - "first_commit_ts" is a new name for the "timestamp" column in the checker table. It shows the first commit from a particular lab and user.
   - "first_view_ts" shows the first time a user visited the pageviews table. It is the timestamp of when a user visited the newsfeed.
   - `status = 'ready'` should still be a filter.
   - `numTrials = 1` should still be a filter.
   - "labnames" should be from the list: `laba04`, `laba04s`, `laba05`, `laba06`, `laba06s`, and `project1`.
   - The table should contain only users (uids with user_*), not admins.
   - "first_commit_ts" and "first_view_ts" should be parsed as `datetime64[ns]`.
3. Using Pandas methods, create two dataframes: `test` and `control`.
   - `test` should contain users with values in "first_view_ts".
   - `control` should contain users with missing values in "first_view_ts".
   - Replace the missing values in the control table with the average "first_view_ts" value of the test users. We will use this value for future analyses.
   - Save both tables in the database. You will use them in the next exercises.
4. Close the connection.

A small piece of advice: do this step by step, from simple to more complex. This will help you debug your queries.

## Chapter VII

### Exercise 03. Aggregations

- Turn-in directory: `ex03/`.
- Files to turn in: `ex03_aggs.ipynb`.
- Allowed functions: `import pandas as pd`, `import sqlite3`.

Our previous work was merely data preparation. We didn't gain any insights from the data. It's time to change that. Remember our hypothesis that users would start working on labs earlier if they saw the newsfeed? The key metric for us is the difference between when a user starts working on a lab (their first commit) and the lab's deadline.

For this exercise, you need to:

1. Create a connection to the database using the `sqlite3` library.
2. Get the schema of the `test` table.
3. Get only the first ten rows of the `test` table to see what it looks like.
4. Find the minimum value of the delta between the first commit and the deadline of the corresponding lab for all users using only one query.
   - Do this by joining the table with the `deadlines` table.
   - The difference should be displayed in hours.
   - Do not take lab `project1` into account; it has longer deadlines and will be an outlier.
   - The value should be stored in the dataframe `df_min` with the corresponding uid.
5. Do the same thing for the maximum, but use only one query. The dataframe name is `df_max`.
6. Do the same thing, but for the average. Use only one query. This time, your dataframe should not include the uid column. The dataframe name is `df_avg`.
7. We want to test the hypothesis that users who visited the newsfeed just a few times have a lower delta between the first commit and the deadline. To do this, calculate the correlation coefficient between the number of pageviews and the difference.
   - Using only one query, create a table with the following columns: "uid", "avg_diff", and "pageviews".
   - "uid" is the uids that exist in the `test`.
   - "avg_diff" is the average delta between the first commit and the lab deadline per user.
   - "pageviews" is the number of Newsfeed visits per user.
   - Do not take the lab `project1` into account.
   - Store it in the dataframe `views_diff`.
   - Use the Pandas `corr()` method to calculate the correlation coefficient between the number of pageviews and the difference.
8. Close the connection.

## Chapter VIII

### Exercise 04. A/B-testing

- Turn-in directory: `ex04/`.
- Files to turn in: `ex04_ab-test.ipynb`.
- Allowed functions: `import pandas as pd`, `import sqlite3`.

So, let's finally find out if the Newsfeed affected the students' behavior. Did they start working on the labs earlier? Remember, we have two tables prepared in the database: test and control. We are going to conduct an A/B test of sorts. First, we need to calculate the difference between the first commit and the deadline before and after they visited the page for the first time. We need to do the same for the control group.

In other words, each user in the test group has their own timestamp for their first visit to the Newsfeed. We want to calculate the average delta (first commit â€” deadline) before and after that timestamp. We will do the same for users in the control group. You might say, "But they did not visit the newsfeed at all". That is correct. Earlier, we decided to use the average timestamp of the first view from the test group for the control group users.

If the delta before the first Newsfeed visit is significantly different compared to the delta afterward in the test group and we don't see the same effect in the control group, then creating the page was a great idea. We can roll it out to the whole group.

In more detail:

1. Create a connection to the database using the `sqlite3` library.
2. Using only one query for each group, create two dataframes: `test_results` and `control_results`, with the columns "time" and "avg_diff", and only two rows.
   - "time" should have the values after and before.
   - "avg_diff" should contain the average delta for all users for the time period before and after their first visit to the page.
   - Only take into account users with observations before and after.
3. We are still not using the lab `project1`.
4. Close the connection.
5. Have the answer: Did the hypothesis turn out to be true, and does the page affect students' behavior?